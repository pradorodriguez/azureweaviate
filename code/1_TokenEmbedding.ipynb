{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token and Embeddings Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "from random import random, randint\n",
    "from math import floor, log\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib as mtplt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text tokenization is a fundamental process in natural language processing (NLP) and information retrieval. \n",
    "* The primary goal of tokenization is to represent text in a manner that's meaningful for machines without losing its context.\n",
    "* It involves breaking down a given text into smaller, meaningful units called tokens. \n",
    "* These tokens can be individual words, phrases, or even sentences, depending on the level of granularity required for analysis or processing.\n",
    "* By converting text into tokens, algorithms can more easily identify patterns, which is crucial for tasks such as sentiment analysis, machine translation, text classification, and keyword extraction23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = model.tokenize([\"The future belongs to those who prepare for it today\"])\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.convert_ids_to_tokens(tokenized_data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour (k-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The main purpose of the k-nearest neighbors (k-NN) algorithm is to classify or predict the value of a data point based on the data points that are closest to it in the feature space. \n",
    "* In other words, it uses the similarity between data points to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_num = 30 # Number of vectors (nodes)\n",
    "dim = 2 ## Dimention. Set to be 2. All the graph plots are for dim 2. If changed, then plots should be commented. \n",
    "m_nearest_neighbor = 2 # M Nearest Neigbor used in construction of the Navigable Small World (NSW)\n",
    "\n",
    "vec_pos = np.random.uniform(size=(vec_num, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query\n",
    "query_vec = [0.5, 0.5]\n",
    "\n",
    "nodes = []\n",
    "nodes.append((\"Q\",{\"pos\": query_vec}))\n",
    "\n",
    "G_query = nx.Graph()\n",
    "G_query.add_nodes_from(nodes)\n",
    "\n",
    "print(\"nodes = \", nodes, flush=True)\n",
    "\n",
    "pos_query=nx.get_node_attributes(G_query,'pos')\n",
    "\n",
    "def nearest_neigbor(vec_pos,query_vec):\n",
    "    nearest_neighbor_index = -1\n",
    "    nearest_dist = float('inf')\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    for i in range(np.shape(vec_pos)[0]):\n",
    "        nodes.append((i,{\"pos\": vec_pos[i,:]}))\n",
    "        if i<np.shape(vec_pos)[0]-1:\n",
    "            edges.append((i,i+1))\n",
    "        else:\n",
    "            edges.append((i,0))\n",
    "\n",
    "        dist = np.linalg.norm(query_vec-vec_pos[i])\n",
    "        if dist < nearest_dist:\n",
    "            nearest_neighbor_index = i\n",
    "            nearest_dist = dist\n",
    "        \n",
    "    G_lin = nx.Graph()\n",
    "    G_lin.add_nodes_from(nodes)\n",
    "    G_lin.add_edges_from(edges)\n",
    "\n",
    "    nodes = []\n",
    "    nodes.append((\"*\",{\"pos\": vec_pos[nearest_neighbor_index,:]}))\n",
    "    G_best = nx.Graph()\n",
    "    G_best.add_nodes_from(nodes)\n",
    "    return G_lin, G_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(G_lin, G_best) = nearest_neigbor(vec_pos,query_vec)\n",
    "\n",
    "pos_lin=nx.get_node_attributes(G_lin,'pos')\n",
    "pos_best=nx.get_node_attributes(G_best,'pos')\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "nx.draw(G_lin, pos_lin, with_labels=True, node_size=150, node_color=[[0.8,0.8,1]], width=0.0, font_size=7, ax = axs)\n",
    "nx.draw(G_query, pos_query, with_labels=True, node_size=200, node_color=[[0.5,0,0]], font_color='white', width=0.5, font_size=7, font_weight='bold', ax = axs)\n",
    "nx.draw(G_best, pos_best, with_labels=True, node_size=200, node_color=[[0.85,0.7,0.2]], width=0.5, font_size=7, font_weight='bold', ax = axs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
